<p align="center"> <h1>üöÄ LOCAL AI ARCHITECTURE: MCP vs. RAG</h1> <h3>Architecting Intelligence from Raw Data Units</h3> </p>

üìå Project Overview
This project demonstrates the end-to-end engineering required to transform raw data into a professional AI knowledge system. Whether working with a custom dataset (in this project: 13,938 data units extracted from UK industry reports) or your own proprietary data, this repository provides the blueprint for two distinct integration paths:

MCP (Model Context Protocol): Direct tool-based connection.

RAG (Retrieval-Augmented Generation): Semantic vector-based memory.

The objective is to guide a Junior Data Analyst through the transition from raw, fragmented files to a production-ready AI context.

<p align="center"> <h2>üõ† THE 5-STAGE ENGINEERING PIPELINE</h2> </p>

<details> <summary><b>Click to expand the 5-Stage Curriculum details</b></summary>

Stage 1: Data Validation & Schema Enforcement
Raw data is often noisy. This stage focuses on Data Wrangling. We implement scripts to ensure every data unit adheres to a strict schema, removing duplicates and correcting encoding errors.

Stage 2: Semantic Metadata Enrichment
To improve retrieval accuracy, data must be contextualized. We programmatically tag chunks with relevant metadata (e.g., Sector, Source, or Region).

Stage 3: Vectorization & Neural Embeddings
Text is converted into high-dimensional vectors. This enables Semantic Search‚Äîfinding information based on meaning rather than just keywords.

Stage 4: Vector Database Implementation
We utilize specialized databases like ChromaDB to store these embeddings locally, optimizing for sub-second retrieval speeds.

Stage 5: Inference & Pipeline Integration
The final stage connects the local inference engine (e.g., Ollama) to the data, allowing models like Qwen or DeepSeek to generate grounded answers.

</details>

<p align="center"> <h2>üîå INTEGRATION PATHS: MCP vs. RAG</h2> </p>

üß© Path A: The MCP Approach (Standardized Connection)
The Model Context Protocol (MCP) allows AI models to securely access local data and tools via a standardized server interface.

The Workflow:

Step 1: Initialize an MCP Server using Python or TypeScript.

Step 2: Expose your data directory as a Resource within the server.

Step 3: Define Tools ‚Äî custom scripts that allow the AI to search or filter through your files (in this project: searching the 139 JSON files).

Step 4: The AI "calls" these tools dynamically to fetch only the necessary data.

üß† Path B: The RAG Approach (Deep Memory)
Retrieval-Augmented Generation (RAG) involves pre-processing data into a "Vector Store" to allow the AI to "remember" vast amounts of information.

The Workflow:

Step 1: Data Ingestion (Chunking ‚Üí Embedding ‚Üí Storage).

Step 2: Vector Indexing (Creating a mathematical map of the data).

Step 3: Semantic Retrieval (Finding data based on the "intent" of a query).

Step 4: Context Injection (Feeding the retrieved data into the LLM prompt).

<p align="center"> <h2>üìä ARCHITECTURAL COMPARISON</h2> </p>

<details> <summary><b>Click to view: Why choose one over the other?</b></summary>

Feature	MCP (Model Context Protocol)	RAG (Retrieval-Augmented)
Data Access	Direct / Real-time	Pre-processed / Vectorized
Setup Complexity	Medium (Server-based)	High (Pipeline-based)
Primary Use Case	Interacting with local files/tools	Querying large knowledge bases
Flexibility	High (Executes local code)	Low (Queries static index)

</details>

<p align="center"> <h2>üöÄ GETTING STARTED: STAGE 1 EXECUTION</h2> </p>

Regardless of the path chosen, Data Validation is the mandatory first step. We must ensure that the dataset (in this project: the 139 "Diamonds" JSON files) is technically sound.

Technical Task: validate_chunks.py
This script acts as the "Gatekeeper" for the pipeline, performing three critical checks:

JSON Syntax Verification: Prevents parsing errors during AI ingestion.

Schema Integrity: Confirms mandatory fields (e.g., text, id) are present.

UTF-8 Compliance: Ensures the text is readable by modern LLMs without encoding crashes.

Python

### validate_chunks.py - Pipeline Initialization
import json
import os

def validate_data_integrity(target_path):
    """Scan and validate JSON files for schema compliance."""
    # Implementation details follow in the Stage 1 module
    pass

if __name__ == "__main__":
    # Path to the data directory (Adjust to your local path)
    DATA_DIR = "./data/chunks" 
    validate_data_integrity(DATA_DIR)

<p align="center">
  <h2>üè∑Ô∏è STAGE 2: SEMANTIC TAXONOMY</h2>
</p>

To transform raw data into a structured knowledge base, we implement a custom enrichment pipeline. This process categorizes each data unit into one of three thematic "Zones" based on specific industry keywords.

| Zone | Category | Logic / Keywords | Goal |
| :--- | :--- | :--- | :--- |
| **Zone A** | **Public & Policy** | Government, NHS, Ethics, Safety | Ensuring AI understands regulatory guardrails. |
| **Zone B** | **Business Impact** | Investment, Market, Revenue, Startups | Focusing on economic growth and industry scale. |
| **Zone C** | **Future of Work** | Skills, Automation, Jobs, Training | Preparing AI for human-centric labor evolution. |

### üìä Enrichment Statistics (Latest Audit)
* **Zone A (Public/Policy):** [Paste Number From Terminal] units
* **Zone B (Business/Impact):** [Paste Number From Terminal] units
* **Zone C (Future of Work):** [Paste Number From Terminal] units
* **Uncategorized:** [Paste Number From Terminal] units

---

### üìÇ Stage 2 Tooling
* **Enrichment Engine:** [`enrich_metadata.py`](./Project_Final/enrich_metadata.py)
* **Audit Utility:** [`audit_enrichment.py`](./Project_Final/audit_enrichment.py)



<p align="center"> <h2>üß† STAGE 3: VECTORIZATION & NEURAL EMBEDDINGS</h2> </p>

Now we enter the most "AI" part of the project. Computers cannot "read" text like we do; they read Numbers. Specifically, they read Vectors.

üîç What happens in Stage 3?
We will take your Enriched JSONs and pass them through a Sentence Transformer model (locally, using your venv).

The Process: Each text_preview is converted into a list of 384 or 768 floating-point numbers (a Vector).

The Goal: If two pieces of data talk about "AI Safety" and "Regulatory Ethics," their vectors will be "mathematically close" to each other. This is how the AI "understands" context.

üõ†Ô∏è The Next Technical Task: create_embeddings.py
This script will be the heaviest one so far. It will:

Load a local embedding model (like all-MiniLM-L6-v2).

Iterate through your PROGRES_SIGUR_ENRICHED folder.

Generate a mathematical representation for every single unit.

Prepare the data for Stage 4 (The Vector Database).
